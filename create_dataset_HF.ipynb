{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b2b9e14",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "889cbbcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total messages: 3088\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def load_and_format_gotquestions(file_path):\n",
    "    \"\"\"\n",
    "    Load the gotquestions JSON file and format it into conversation pairs.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the JSON file\n",
    "        \n",
    "    Returns:\n",
    "        list: List of conversation dictionaries with 'content' and 'role' keys\n",
    "    \"\"\"\n",
    "    # Load the JSON file\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    conversations = []\n",
    "    \n",
    "    # Iterate through each category\n",
    "    for category in data:\n",
    "        category_name = category.get('name', '')\n",
    "        articles = category.get('articles', [])\n",
    "        \n",
    "        # Process each article in the category\n",
    "        for article in articles:\n",
    "            question = article.get('name', '')\n",
    "            answer = article.get('answer', '')\n",
    "            \n",
    "            # Clean up the answer by removing extra whitespace and newlines\n",
    "            answer = answer.strip()\n",
    "            \n",
    "            # Create conversation pair\n",
    "            conversation_pair = [\n",
    "                {\"role\": \"user\", \"content\": question},\n",
    "                {\"role\": \"assistant\", \"content\": answer}\n",
    "            ]\n",
    "            \n",
    "            conversations.append(conversation_pair)\n",
    "    \n",
    "    return conversations\n",
    "\n",
    "def load_and_format_qa_messages_jsonl(file_path):\n",
    "    \"\"\"\n",
    "    Load the Arabic Final qa_messages.jsonl file and format it into conversation pairs.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the JSONL file\n",
    "\n",
    "    Returns:\n",
    "        list: List of conversation pairs (each pair is a list of dicts with 'role' and 'content')\n",
    "    \"\"\"\n",
    "    conversations = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line.strip())\n",
    "            msgs = data.get(\"messages\", [])\n",
    "            # Only keep user/assistant pairs (ignore system if present)\n",
    "            pair = []\n",
    "            for msg in msgs:\n",
    "                if msg[\"role\"] in (\"user\", \"assistant\"):\n",
    "                    pair.append({\"role\": msg[\"role\"], \"content\": msg[\"content\"]})\n",
    "            if len(pair) == 2:\n",
    "                conversations.append(pair)\n",
    "    return conversations\n",
    "\n",
    "# Load as one long conversation\n",
    "file_path = \"data/gotquestions_ar.json\"\n",
    "formatted_data = load_and_format_gotquestions(file_path)\n",
    "\n",
    "file_path_jsonl = \"data/Arabic Final qa_messages.jsonl\"\n",
    "formatted_data_jsonl = load_and_format_qa_messages_jsonl(file_path_jsonl)\n",
    "\n",
    "# Combine both datasets\n",
    "combined_data = formatted_data + formatted_data_jsonl\n",
    "\n",
    "# Randomly shuffle the combined data\n",
    "# random.shuffle(combined_data)\n",
    "print(f\"Total messages: {len(combined_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0212646f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "الحمد لله رب العالمين، والصلاة والسلام على سيدنا محمد وعلى آله وصحبه أجمعين.\n",
      "\n",
      "(All praise is due to Allah, Lord of the worlds, and blessings and peace be upon our master Muhammad and his family and companions.)\n",
      "\n",
      "السؤال عن \"غاية الإنسان\" سؤال مهم وعميق. الكتاب المقدس يجيب على هذا السؤال بوضوح في رسالة يعقوب.\n",
      "\n",
      "**\"ليس الغرض من الإنسان هو أن يكون مسرورًا، بل أن يمجد الله\" (يعقوب 4:2).**\n",
      "\n",
      "هذا يعني أن هدفنا الأسمى كبشر ليس البحث عن السعادة أو النجاح الشخصي، بل أن نعيش حياة تمجد الله في كل ما نفعله. وهذا يشمل:\n",
      "\n",
      "*   **معرفة الله:** كلما عرفنا الله أكثر، كلما زادت رغبتنا في تمجيده. (يوحنا 17:3)\n",
      "*   **محبة الله:** محبة الله هي جوهر الشريعة الجديدة (متى 22:37-38).\n",
      "*   **طاعة الله:** طاعة وصايا الله هي تعبير عن محبتنا له. (يوحنا 14:15)\n",
      "*   **العيش بمجد لله:** أن نكون نورًا في هذا العالم، وأن نعلن عن محبته وغفرانه للآخرين. (متى 5:16)\n",
      "\n",
      "هذا لا يعني أن السعادة ليست مهمة. بل على العكس، السعادة الحقيقية والدائمة تأتي من العيش في علاقة مع الله ومن تمجيده في حياتنا. فالله هو مصدر كل خير وسعادة. \n",
      "\n",
      "أتمنى أن يكون هذا التوضيح مفيدًا. هل هناك أي شيء آخر يمكنني المساعدة فيه؟\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import InferenceClient\n",
    "import parrot_ai.prompts as parrot_prompts\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "client = InferenceClient(\n",
    "    provider=\"nebius\",\n",
    "    api_key=os.environ[\"HF_TOKEN\"],\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"google/gemma-3-27b-it\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": parrot_prompts.MAIN_SYSTEM_PROMPT\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the chief end of man?\"\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e245dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f1fde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import parrot_ai.prompts as parrot_prompts\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Output file for the training dataset\n",
    "output_file = \"data/training_dataset.jsonl\"\n",
    "\n",
    "# Check if the output file exists and count existing entries\n",
    "existing_count = 0\n",
    "if os.path.exists(output_file):\n",
    "    with open(output_file, 'r', encoding='utf-8') as f:\n",
    "        existing_count = sum(1 for line in f if line.strip())\n",
    "    print(f\"Found existing file with {existing_count} entries. Resuming from there.\")\n",
    "else:\n",
    "    print(\"Creating new training dataset file.\")\n",
    "\n",
    "# Process data starting from where we left off\n",
    "start_index = existing_count\n",
    "total_data = len(combined_data)\n",
    "\n",
    "print(f\"Processing {total_data - start_index} remaining entries...\")\n",
    "\n",
    "# Open file in append mode for incremental saving\n",
    "with open(output_file, 'a', encoding='utf-8') as f:\n",
    "    for i in tqdm(range(start_index, total_data), desc=\"Generating training data\"):\n",
    "        try:\n",
    "            data = combined_data[i]\n",
    "            \n",
    "            # Generate response using parrot_chain\n",
    "            response = parrot_chain(data, parrot)\n",
    "            \n",
    "            # Create training example in the format expected for fine-tuning\n",
    "            training_example = {\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": parrot_prompts.MAIN_SYSTEM_PROMPT\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": data[0][\"content\"]  # User question\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": response[\"final_answer\"]  # Final answer from chain\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            # Write the training example as a JSON line\n",
    "            f.write(json.dumps(training_example, ensure_ascii=False) + '\\n')\n",
    "            f.flush()  # Ensure data is written immediately\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError processing item {i}: {e}\")\n",
    "            print(f\"Question: {data[0]['content'][:100]}...\")\n",
    "            # Continue with the next item instead of stopping\n",
    "            continue\n",
    "\n",
    "print(f\"\\nTraining dataset creation completed!\")\n",
    "print(f\"Output saved to: {output_file}\")\n",
    "\n",
    "# Count final entries\n",
    "with open(output_file, 'r', encoding='utf-8') as f:\n",
    "    final_count = sum(1 for line in f if line.strip())\n",
    "print(f\"Total training examples: {final_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bfa18c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arabic_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
