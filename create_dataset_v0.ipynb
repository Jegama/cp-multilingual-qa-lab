{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b2b9e14",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889cbbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_and_format_gotquestions(file_path):\n",
    "    \"\"\"\n",
    "    Load the gotquestions JSON file and format it into conversation pairs.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the JSON file\n",
    "        \n",
    "    Returns:\n",
    "        list: List of conversation dictionaries with 'content' and 'role' keys\n",
    "    \"\"\"\n",
    "    # Load the JSON file\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    conversations = []\n",
    "    \n",
    "    # Iterate through each category\n",
    "    for category in data:\n",
    "        category_name = category.get('name', '')\n",
    "        articles = category.get('articles', [])\n",
    "        \n",
    "        # Process each article in the category\n",
    "        for article in articles:\n",
    "            question = article.get('name', '')\n",
    "            answer = article.get('answer', '')\n",
    "            \n",
    "            # Clean up the answer by removing extra whitespace and newlines\n",
    "            answer = answer.strip()\n",
    "            \n",
    "            # Create conversation pair\n",
    "            conversation_pair = [\n",
    "                {\"role\": \"user\", \"content\": question},\n",
    "                {\"role\": \"assistant\", \"content\": answer}\n",
    "            ]\n",
    "            \n",
    "            conversations.append(conversation_pair)\n",
    "    \n",
    "    return conversations\n",
    "\n",
    "def load_and_format_qa_messages_jsonl(file_path):\n",
    "    \"\"\"\n",
    "    Load the Arabic Final qa_messages.jsonl file and format it into conversation pairs.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the JSONL file\n",
    "\n",
    "    Returns:\n",
    "        list: List of conversation pairs (each pair is a list of dicts with 'role' and 'content')\n",
    "    \"\"\"\n",
    "    conversations = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line.strip())\n",
    "            msgs = data.get(\"messages\", [])\n",
    "            # Only keep user/assistant pairs (ignore system if present)\n",
    "            pair = []\n",
    "            for msg in msgs:\n",
    "                if msg[\"role\"] in (\"user\", \"assistant\"):\n",
    "                    pair.append({\"role\": msg[\"role\"], \"content\": msg[\"content\"]})\n",
    "            if len(pair) == 2:\n",
    "                conversations.append(pair)\n",
    "    return conversations\n",
    "\n",
    "# Load as one long conversation\n",
    "file_path = \"data/gotquestions_ar.json\"\n",
    "formatted_data = load_and_format_gotquestions(file_path)\n",
    "\n",
    "file_path_jsonl = \"data/Arabic Final qa_messages.jsonl\"\n",
    "formatted_data_jsonl = load_and_format_qa_messages_jsonl(file_path_jsonl)\n",
    "\n",
    "# Combine both datasets\n",
    "combined_data = formatted_data + formatted_data_jsonl\n",
    "\n",
    "# Randomly shuffle the combined data\n",
    "# random.shuffle(combined_data)\n",
    "print(f\"Total messages: {len(combined_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0212646f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from parrot_ai import ParrotAI, parrot_chain\n",
    "\n",
    "parrot = ParrotAI()\n",
    "parrot.load_model(\"google/gemma-3-12b-it\")\n",
    "\n",
    "# Check if model is loaded\n",
    "if parrot.is_loaded():\n",
    "    print(parrot.get_model_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f1fde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import parrot_ai.prompts as parrot_prompts\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Output file for the training dataset\n",
    "output_file = \"data/training_dataset.jsonl\"\n",
    "\n",
    "# Check if the output file exists and count existing entries\n",
    "existing_count = 0\n",
    "if os.path.exists(output_file):\n",
    "    with open(output_file, 'r', encoding='utf-8') as f:\n",
    "        existing_count = sum(1 for line in f if line.strip())\n",
    "    print(f\"Found existing file with {existing_count} entries. Resuming from there.\")\n",
    "else:\n",
    "    print(\"Creating new training dataset file.\")\n",
    "\n",
    "# Process data starting from where we left off\n",
    "start_index = existing_count\n",
    "total_data = len(combined_data)\n",
    "\n",
    "print(f\"Processing {total_data - start_index} remaining entries...\")\n",
    "\n",
    "# Open file in append mode for incremental saving\n",
    "with open(output_file, 'a', encoding='utf-8') as f:\n",
    "    for i in tqdm(range(start_index, total_data), desc=\"Generating training data\"):\n",
    "        try:\n",
    "            data = combined_data[i]\n",
    "            \n",
    "            # Generate response using parrot_chain\n",
    "            response = parrot_chain(data, parrot)\n",
    "            \n",
    "            # Create training example in the format expected for fine-tuning\n",
    "            training_example = {\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": parrot_prompts.MAIN_SYSTEM_PROMPT\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": data[0][\"content\"]  # User question\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": response[\"final_answer\"]  # Final answer from chain\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            # Write the training example as a JSON line\n",
    "            f.write(json.dumps(training_example, ensure_ascii=False) + '\\n')\n",
    "            f.flush()  # Ensure data is written immediately\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError processing item {i}: {e}\")\n",
    "            print(f\"Question: {data[0]['content'][:100]}...\")\n",
    "            # Continue with the next item instead of stopping\n",
    "            continue\n",
    "\n",
    "print(f\"\\nTraining dataset creation completed!\")\n",
    "print(f\"Output saved to: {output_file}\")\n",
    "\n",
    "# Count final entries\n",
    "with open(output_file, 'r', encoding='utf-8') as f:\n",
    "    final_count = sum(1 for line in f if line.strip())\n",
    "print(f\"Total training examples: {final_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bfa18c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arabic_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
